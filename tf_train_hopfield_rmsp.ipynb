{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield Neural Network 5x5 RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pylab import imshow, cm, show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String versions of letters that will be learned by network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"\"\"\n",
    ".XXX.\n",
    "X...X\n",
    "XXXXX\n",
    "X...X\n",
    "X...X\n",
    "\"\"\"\n",
    "\n",
    "Z = \"\"\"\n",
    "XXXXX\n",
    "...X.\n",
    "..X..\n",
    ".X...\n",
    "XXXXX\n",
    "\"\"\"\n",
    "\n",
    "E = \"\"\"\n",
    "XXXXX\n",
    "X....\n",
    "XXXX.\n",
    "X....\n",
    "XXXXX\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patternization and visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pattern(letter):\n",
    "    return np.reshape(np.array([+1 if c=='X' else -1 for c in letter.replace('\\n','')], dtype=np.float32), [1,25])\n",
    "\n",
    "def display(pattern):\n",
    "    imshow(pattern.reshape((5,5)),cmap=cm.binary, interpolation='nearest')\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see the whole future memory set of network\n",
    "Just to be sure, if the functions work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(to_pattern(A))\n",
    "display(to_pattern(Z))\n",
    "display(to_pattern(E))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_patterns_prev = np.vstack([to_pattern(A), to_pattern(Z)])\n",
    "t_patterns = np.vstack([t_patterns_prev, to_pattern(E)])\n",
    "print('Patterns shape: '+str(t_patterns.shape))\n",
    "print('Patterns:\\n'+str(t_patterns))\n",
    "\n",
    "def init(patterns):\n",
    "    r,c = patterns.shape\n",
    "    w = np.round(np.random.rand(c,c), decimals=3)\n",
    "    w[np.diag_indices(c)] = 0\n",
    "    return w\n",
    "\n",
    "W = tf.Variable(init(t_patterns), dtype=tf.float32, name='w')\n",
    "W_ = init(t_patterns)\n",
    "\n",
    "print('Shape of weight matrix: '+str(W_.shape))\n",
    "print('Weights:\\n'+str(W_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "### We will use not only one parasitic energy point of a memory, but 5 at once and try to maximize them, while decreasing the energy of target memories\n",
    "We will use sigmoid function and then round results, because it is a binary decision for a neural network. Of couse it seems to be more legit to use the 1/-1 versions of final neuron activations as we used in making patterns from letters, but we cannot round tanh function results properly without getting zeros and apply special functions to every scalar every time without hardcoding and using boolean logic. It does not matter, if we will suddenly change the whole perspective in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [1, 25], name='X')\n",
    "\n",
    "with tf.name_scope('Y_1'):\n",
    "    Y_prev_1 = tf.matmul(X,W)\n",
    "    Y_1 = tf.round(tf.nn.sigmoid(Y_prev_1))\n",
    "\n",
    "with tf.name_scope('Y_2'):\n",
    "    Y_prev_2 = tf.matmul(Y_prev_1, W)\n",
    "    Y_2 = tf.round(tf.nn.sigmoid(Y_prev_2))\n",
    "\n",
    "with tf.name_scope('Y_3'):\n",
    "    Y_prev_3 = tf.matmul(Y_prev_2, W)\n",
    "    Y_3 = tf.round(tf.nn.sigmoid(Y_prev_3))\n",
    "\n",
    "with tf.name_scope('Y_4'):\n",
    "    Y_prev_4 = tf.matmul(Y_prev_3, W)\n",
    "    Y_4 = tf.round(tf.nn.sigmoid(Y_prev_4))\n",
    "\n",
    "with tf.name_scope('Y_5'):\n",
    "    Y_prev_5 = tf.matmul(Y_prev_4, W)\n",
    "    Y_5 = tf.round(tf.nn.sigmoid(Y_prev_5))\n",
    "\n",
    "with tf.name_scope('t_energy'):\n",
    "    target_energy = tf.reshape(-0.5*tf.matmul(tf.matmul(X, W), X, transpose_b=True), [])\n",
    "    \n",
    "with tf.name_scope('p_energy'):\n",
    "    with tf.name_scope('p_energy_1'):\n",
    "        parasitic_energy_1 = tf.reshape(-0.5*tf.matmul(tf.matmul(Y_1,W),Y_1, transpose_b=True), [])\n",
    "    with tf.name_scope('p_energy_2'):\n",
    "        parasitic_energy_2 = tf.reshape(-0.5*tf.matmul(tf.matmul(Y_2,W),Y_2, transpose_b=True), [])\n",
    "    with tf.name_scope('p_energy_3'):\n",
    "        parasitic_energy_3 = tf.reshape(-0.5*tf.matmul(tf.matmul(Y_3,W),Y_3, transpose_b=True), [])\n",
    "    with tf.name_scope('p_energy_4'):\n",
    "        parasitic_energy_4 = tf.reshape(-0.5*tf.matmul(tf.matmul(Y_4,W),Y_4, transpose_b=True), [])\n",
    "    with tf.name_scope('p_energy_5'):\n",
    "        parasitic_energy_5 = tf.reshape(-0.5*tf.matmul(tf.matmul(Y_5,W),Y_5, transpose_b=True), [])\n",
    "    with tf.name_scope('p_m_energy'):\n",
    "        parasitic_mean_energy = tf.reshape(tf.divide(tf.add_n([parasitic_energy_1, parasitic_energy_2, parasitic_energy_3, parasitic_energy_4, parasitic_energy_5]), 5.0), [])\n",
    "\n",
    "tf.summary.scalar('t_energy_sc', target_energy)\n",
    "tf.summary.scalar('p_energy_1_sc', parasitic_energy_1)\n",
    "tf.summary.scalar('p_energy_2_sc', parasitic_energy_2)\n",
    "tf.summary.scalar('p_energy_3_sc', parasitic_energy_3)\n",
    "tf.summary.scalar('p_energy_4_sc', parasitic_energy_4)\n",
    "tf.summary.scalar('p_energy_5_sc', parasitic_energy_5)\n",
    "tf.summary.scalar('p_m_energy_sc', parasitic_mean_energy)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01, decay=0.001).minimize(target_energy-parasitic_mean_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Session\n",
    "### There is no point to run it for a very long time because you will almost never delete all parasitic memories by decreasing energy\n",
    "So if you have a much more difficult task it is better to teach network to escape from parasites, not decrease energy level of part of them, but it surely can help a little bit. The main idea is that you should use Boltzmann Machines (basically Hopfield Nets with probability), if deterministic version of Hopfield Net has failed, not overfit the original Hopfield Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initializers.global_variables())\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./graph', sess.graph)\n",
    "    for i in xrange(10000):\n",
    "        if i == 0:\n",
    "            num = 1\n",
    "        _, t_e_1, p_e_1, summary_1 = sess.run([optimizer, target_energy, parasitic_mean_energy, merged], feed_dict={X: to_pattern(A)})\n",
    "        print('Step '+str(num)+':')\n",
    "        print('Target energy of A: '+str(t_e_1))\n",
    "        print('   Parasitic energy of A: '+str(p_e_1))\n",
    "        writer.add_summary(summary_1, num)\n",
    "        num+=1\n",
    "        _, t_e_2, p_e_2, summary_2 = sess.run([optimizer, target_energy, parasitic_mean_energy, merged], feed_dict={X: to_pattern(Z)})\n",
    "        print('Step '+str(num)+':')\n",
    "        print('Target energy of Z: '+str(t_e_2))\n",
    "        print('   Parasitic energy of Z: '+str(p_e_2))\n",
    "        writer.add_summary(summary_2, num)\n",
    "        num+=1\n",
    "        _, t_e_3, p_e_3, summary_3 = sess.run([optimizer, target_energy, parasitic_mean_energy, merged], feed_dict={X: to_pattern(E)})\n",
    "        print('Step '+str(num)+':')\n",
    "        print('Target energy of E: '+str(t_e_3))\n",
    "        print('   Parasitic energy of E: '+str(p_e_3))\n",
    "        writer.add_summary(summary_3, num)\n",
    "        num+=1\n",
    "    weights = W.eval()\n",
    "    print('Weights of trained model:\\n'+str(weights))\n",
    "    save_path = saver.save(sess, './model/hopfield')\n",
    "    print('Model has been successfully trained and saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
